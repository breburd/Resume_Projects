{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework Assignment 5 : Breanna Burd\n",
    "#### bburd1@jh.edu\n",
    "#### (570)-854-7060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> __Johns Hopkins University__</h3>\n",
    "## <h3 align=\"center\">__Whiting School of Engineering__</h3>\n",
    "## <h3 align=\"center\">__Engineering for Professionals__</h3>\n",
    "## <h3 align=\"center\">__685.621 Algorithms for Data Science__</h3>\n",
    "## <h3 align=\"center\">__Homework 5__</h3>\n",
    "## <h3 align=\"center\">__Assigned at the start of Module 12__</h3>\n",
    "## <h3 align=\"center\">__Due at the end of Module 13__</h3><br>\n",
    "## <h3 align=\"center\">__Total Points 100/100__</h3>\n",
    "Class, the below is a standard set of instructions for each HW, in this assignment groups will be set up for collaboration.<br><br>\n",
    "Make sure your group starts one thread for the collaborative problems. You are required to participate in the collaborative problem and subproblem separately. Please do not directly post a complete\n",
    "solution, the goal is for the group to develop a solution after everyone has participated. Please ensure\n",
    "you have a write-up with solutions to each problem and subproblems, you are also required to submit\n",
    "code that will be compiled when grading the assignment. In each of the problems you are allowed to\n",
    "use built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __1 - Module 13 Note this is Collaborative Problem - Note: create threads for both subparts, you are required to participate in both subparts.__<br>\n",
    "*50 Points Total*<br><br>\n",
    "In this problem you only have to complete one of the two following problems:<br><br>\n",
    "\n",
    "1. In this problem you will use a built-in Feed Forward Neural Network (FFNN).\n",
    "2. In this problem you will use a built-in Convolutional Neural Network (CNN).<br><br>\n",
    "\n",
    "Use either the Iris or numerical data sets and show the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import keras\n",
    "import cvxopt\n",
    "import pandas\n",
    "import numpy as np\n",
    "# imports used for the Feed Forward Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "# imports used for the CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load dataset\n",
    "iris_data = pandas.read_csv('iris.csv', sep=',')\n",
    "# Read in the MNIST dataset as a numpy array\n",
    "mnist_data = pandas.read_csv('train.csv', sep=',').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.10526315789474\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "X = iris_data.iloc[:, 0:4]\n",
    "y = iris_data.iloc[:, 4]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# I set the random_state so my results can be reproduced across multiple iterations of testing\n",
    "random_state = 5\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n",
    "\n",
    "# Standardize the data before feeding it to the neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an MLPClassifier because that is an example of a Feed Forward Neural Network\n",
    "ffnn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state)\n",
    "ffnn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the neural network to make predictions\n",
    "y_pred = ffnn.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Extracted from HW 3. Transform the MNIST Dataset into 28x28 images with a label column\n",
    "# Create a DataFrame with a label column and the 28x28 image column\n",
    "mnist_df = pandas.DataFrame(columns=[\"Label\", \"Image\"])\n",
    "# Only use the first 5,000 data points because the 42,000 takes a very long time\n",
    "number_of_data_points = len(mnist_data)\n",
    "X = []\n",
    "for i in range(0, number_of_data_points):\n",
    "    entry = mnist_data[i]\n",
    "    label = entry[0]\n",
    "    image = np.array(entry[1:]).reshape(28,28)\n",
    "    X.append(image)\n",
    "    mnist_df.loc[i] = [label, image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "493/493 [==============================] - 227s 456ms/step - loss: 0.2480 - accuracy: 0.9227 - val_loss: 0.0910 - val_accuracy: 0.9719\n",
      "Epoch 2/5\n",
      "493/493 [==============================] - 203s 412ms/step - loss: 0.0666 - accuracy: 0.9791 - val_loss: 0.0556 - val_accuracy: 0.9822\n",
      "Epoch 3/5\n",
      "493/493 [==============================] - 198s 401ms/step - loss: 0.0457 - accuracy: 0.9855 - val_loss: 0.0565 - val_accuracy: 0.9835\n",
      "Epoch 4/5\n",
      "493/493 [==============================] - 206s 418ms/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 0.0479 - val_accuracy: 0.9839\n",
      "Epoch 5/5\n",
      "493/493 [==============================] - 234s 476ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0441 - val_accuracy: 0.9881\n",
      "329/329 [==============================] - 6s 19ms/step - loss: 0.0441 - accuracy: 0.9881\n",
      "Test accuracy: 98.8095223903656\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X).reshape(number_of_data_points, 28, 28, 1)\n",
    "y = mnist_df.iloc[:, 0]\n",
    "\n",
    "# I set the random_state so my results can be reproduced across multiple iterations of testing\n",
    "random_state = 5\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n",
    "\n",
    "# Standardize the data before feeding it to the neural network\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "# Add several layers to the CNN\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "pred_loss, pred_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {pred_accuracy * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __2 - Module 12 Optimization - Note this is a Collaborative Problem__<br>\n",
    "*50 points total*\n",
    "\n",
    "In this problem, you will develop the Support Vector Machine (SVM) algorithm from scratch to classify the Iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [10 points] Using the SVM in the Optimization course notes, develop psuedocode for an SVM classifier using a linear and separately an rbf kernel.<br><br>\n",
    "2. [Optional] no need to discuss collaboratively - Analyze the runtime of your design in big O notation and calculate a total runtime such that each line of psuedocode is accounted for.<br><br>\n",
    "3. [40 points] Implement your SVM using Python:<br><br>\n",
    "    - Train three two class models using the Iris dataset as input training data, the Iris data will need to be reconfigured as a one vs. all or one vs. one data set.\n",
    "    - Process the test data set to determine which class each test observation belongs to, in this problem you will simply use all 150 observations as your test data.\n",
    "    - What is the classification accuracy of your design?\n",
    "    - Is there a difference in performance between the two kernels? Why do you think that is?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "# The psuedocode is derrived from the MATLAB classifySVM.m and kernelSVM.m codes provided with module 11\n",
    "\n",
    "def kernel(type, arg):\n",
    "    if type == 'linear':\n",
    "        return X1'*X2\n",
    "    else if type == 'rbf':\n",
    "        return return np.exp(-0.5 * np.sum((X1 - X2)**2) / arg**2)\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def kernel_svm(X1, X2, ker, arg):\n",
    "    dim1, numData1 = X1.shape\n",
    "    dim2, numData2 = X2.shape\n",
    "\n",
    "    K = array_of_zeros((numData1, numData2))\n",
    "    for i from 0 to numData1:\n",
    "        for j from 0 to numData2:\n",
    "            # Calculate the value based on the type of kernel\n",
    "            K[i,j] = kernel(X1[:,i],X2[:,j], ker, arg)\n",
    "    return K\n",
    "\n",
    "def train_svm(X,y,options):\n",
    "    ker, arg. C = options\n",
    "    \n",
    "    dim, numData = X.shape\n",
    "    y = y.flatten()\n",
    "    # Replace all of the other classes with values of -1\n",
    "    y[indeces_where(y == 2)] = -1\n",
    "    \n",
    "    norm = 1\n",
    "    mu = 1e-12\n",
    "    eps = 1e-5  # I changed this from 1e-12 because I was not getting any observations removed...\n",
    "\n",
    "    K = kernel_svm(X,X,ker, arg)  # compute kernel matrix\n",
    "    H = K * (y * y.T)\n",
    "    H = H + (mu * eye(H.shape[0]))  # add small numbers to diagonal \n",
    "\n",
    "    # Use a solver for quadratic programming to find the alpha values\n",
    "    sol = solve_quadratic_programming(H, y)\n",
    "    \n",
    "    alphas = sol.alphas\n",
    "    alpha = alphas.flatten()\n",
    "\n",
    "    # Find support vectors\n",
    "    sv_inx = indeces_where(alpha > eps)[0]\n",
    "    \n",
    "    # Calculate bias term\n",
    "    b = 0\n",
    "    if len(sv_inx) > 0:\n",
    "        # Formula from MATLAB b = sum(y(boundary_inx)-H(boundary_inx,sv_inx)*alpha(sv_inx).*y(boundary_inx))/length( boundary_inx );\n",
    "        b = np.sum(y[sv_inx] - np.dot(H[sv_inx, sv_inx], (alpha[sv_inx] * y[sv_inx]))) / len(sv_inx)\n",
    "    else:\n",
    "        b = 0\n",
    "\n",
    "    prediction = (K *alpha)  + b\n",
    "    tmp = array_of_ones((numData, 1))\n",
    "    # Set elements to -1 where prediction is less than 0\n",
    "    tmp[prediction < 0] = -1\n",
    "    err = (np.sum((abs(tmp-y)))/numData)*100\n",
    "\n",
    "    # alpha = alphas * y\n",
    "    # Return a model with all of the needed properties\n",
    "    return model(alpha[sv_inx], b, options, X[:, sv_inx], err, y[sv_inx], sv_inx, len(sv_inx))\n",
    "\n",
    "def classify_svm(X, model):\n",
    "    # Retrieve all of the values saved from the model\n",
    "    alpha = model.alpha\n",
    "    b = model.b\n",
    "    svX = model.svX\n",
    "    ker = model.options.ker\n",
    "    arg = model.options.arg\n",
    "    \n",
    "    K = kernel_svm(X, svX, ker, arg)\n",
    "    prediction = (K * alpha) + b\n",
    "    num_data = prediction.shape\n",
    "\n",
    "    y_predict = array_of_ones(num_data)\n",
    "    indx = indeces_where(prediction < 0)[0]\n",
    "    y_predict[indx] = 2\n",
    "\n",
    "    return y_predict, prediction\n",
    "\n",
    "def svm(x, y, ker):\n",
    "    one_vs_all_classifications = list of one vs all classification for each class (i.e for 3 classes, there should be 3 arrays in this list)\n",
    "    \n",
    "    X = the columns in x to be tested\n",
    "\n",
    "    sv_indx = []  # support vectors index\n",
    "\n",
    "    # set the options\n",
    "    arg = 0.15  # kernel argument\n",
    "    C = 3  # regularization constant\n",
    "    options = (ker, arg, C)\n",
    "\n",
    "    predictions = [] # Create a list to store all of the model predictions and classifications\n",
    "    for classification in one_vs_all_classifications:\n",
    "        model = train_svm(X.t, classification, optio\n",
    "        yp, prediction = classify_svm(X.T, model)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Combine predictions and find the class with the maximum decision value\n",
    "    ypred = max(predictions)\n",
    "    accuracy = (sum(ypred == y) / 150) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type the response for optional part 2 here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    def __init__(self, alpha, b, options, svX, err, y, sv_inx, numberSV):\n",
    "        self.alpha = alpha\n",
    "        self.b = b\n",
    "        self.options = options\n",
    "        self.svX = svX\n",
    "        self.err = err\n",
    "        self.sv_y = y\n",
    "        self.sv_indx = sv_inx\n",
    "        self.numberSV = numberSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class options(object):\n",
    "    def __init__(self, ker, arg, C):\n",
    "        self.ker = ker\n",
    "        self.arg = arg\n",
    "        self.C = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def kernel(X1,X2,ker, arg):\n",
    "    if ker == 'linear':\n",
    "        # linear kernel -- k(X1,X2) = X1'*X2\n",
    "        return np.dot(X1.T, X2)\n",
    "    elif ker == 'rbf':\n",
    "        # radial basis kernel -- arg[1] = sigma, k(X1,X2) = exp(-0.5*||X1-X2||^2/arg[1]^2) -> arg = 1\n",
    "        return np.exp(-0.5 * np.sum((X1 - X2)**2) / arg**2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_svm(X1, X2, ker, arg):\n",
    "    dim1, numData1 = X1.shape\n",
    "    dim2, numData2 = X2.shape\n",
    "\n",
    "    K = np.zeros((numData1, numData2))\n",
    "    for i in range(numData1):\n",
    "        for j in range(numData2):\n",
    "            K[i,j] = kernel(X1[:,i],X2[:,j], ker, arg)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is derrived from trainSVM.m from the MATLAB code provided in module 11\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def train_svm(X,y,options):\n",
    "    ker = options.ker\n",
    "    arg = options.arg\n",
    "    C = options.C\n",
    "    \n",
    "    dim, numData = X.shape\n",
    "    y = y.flatten()\n",
    "    y[np.where(y == 2)] = -1\n",
    "    \n",
    "    norm = 1\n",
    "    mu = 1e-12\n",
    "    eps = 1e-5  # I changed this from 1e-12 because I was not getting any observations removed...\n",
    "\n",
    "    K = kernel_svm(X,X,ker, arg)  # compute kernel matrix\n",
    "    H = K * (np.outer(y, y))\n",
    "    H = H + (mu * np.eye(H.shape[0]))  # add small numbers to diagonal \n",
    "\n",
    "    P = matrix(H)\n",
    "    q = matrix(-np.ones(numData))\n",
    "    Aeq = matrix(y.reshape(1, -1))  # Minimizes 1/2*X'*H*X + f'*X \n",
    "    beq = matrix(np.zeros((1)))  # subject to Aeq*X = beq\n",
    "    # # X, so that the solution is in the range lb <= X <= ub\n",
    "    lb = np.zeros((numData))  # 0 <= Alpha\n",
    "    ub = np.dot(np.ones((numData)), C)  # This creates an L1-soft margin\n",
    "    G = matrix(np.vstack([ np.eye(numData)*-1, np.eye(numData)]))\n",
    "    h = matrix(np.hstack([ lb, ub]))\n",
    "\n",
    "    sol = solvers.qp(P, q, G, h, Aeq, beq)\n",
    "    \n",
    "    alphas = np.array(sol['x'])\n",
    "    alpha = alphas.flatten()\n",
    "\n",
    "    # Find support vectors\n",
    "    sv_inx = np.where(alpha > eps)[0]\n",
    "    \n",
    "    # Calculate bias term\n",
    "    b = 0\n",
    "    if len(sv_inx) > 0:\n",
    "        # Formula from MATLAB b = sum(y(boundary_inx)-H(boundary_inx,sv_inx)*alpha(sv_inx).*y(boundary_inx))/length( boundary_inx );\n",
    "        b = np.sum(y[sv_inx] - np.dot(H[sv_inx, sv_inx], (alpha[sv_inx] * y[sv_inx]))) / len(sv_inx)\n",
    "    else:\n",
    "        b = 0\n",
    "\n",
    "    prediction = np.dot(K, alpha) + b\n",
    "    tmp = np.ones((numData, 1))\n",
    "    # Set elements to -1 where prediction is less than 0\n",
    "    tmp[prediction < 0] = -1\n",
    "    err = (np.sum((abs(tmp-y)))/numData)*100\n",
    "\n",
    "    # alpha = alphas * y\n",
    "    return model(alpha[sv_inx], b, options, X[:, sv_inx], err, y[sv_inx], sv_inx, len(sv_inx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_svm(X, model):\n",
    "    alpha = model.alpha\n",
    "    b = model.b\n",
    "    svX = model.svX\n",
    "    ker = model.options.ker\n",
    "    arg = model.options.arg\n",
    "    \n",
    "    K = kernel_svm(X, svX, ker, arg)\n",
    "    prediction = np.dot(K, alpha) + b\n",
    "    num_data = prediction.shape\n",
    "\n",
    "    y_predict = np.ones(num_data)\n",
    "    indx = np.where(prediction < 0)[0]\n",
    "    y_predict[indx] = 2\n",
    "\n",
    "    return y_predict, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8757e+01 -1.3595e+03  5e+03  1e+00  1e-13\n",
      " 1: -1.6240e+01 -4.7380e+02  8e+02  1e-01  7e-14\n",
      " 2: -1.1001e+00 -8.7219e+01  1e+02  2e-02  5e-14\n",
      " 3:  1.4555e+00 -1.8208e+01  3e+01  3e-03  2e-14\n",
      " 4: -1.8067e-02 -1.7508e+00  2e+00  2e-05  4e-15\n",
      " 5: -4.0580e-01 -1.3599e+00  1e+00  1e-05  3e-15\n",
      " 6: -7.6187e-01 -1.5177e+00  8e-01  5e-06  3e-15\n",
      " 7: -1.1654e+00 -1.1947e+00  3e-02  9e-08  5e-15\n",
      " 8: -1.1764e+00 -1.1767e+00  3e-04  9e-10  4e-15\n",
      " 9: -1.1765e+00 -1.1765e+00  3e-06  9e-12  5e-15\n",
      "10: -1.1765e+00 -1.1765e+00  3e-08  9e-14  5e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.3214e+02 -1.2852e+03  2e+03  6e-01  3e-13\n",
      " 1: -2.3700e+02 -4.7142e+02  2e+02  3e-14  3e-13\n",
      " 2: -2.8296e+02 -3.2769e+02  4e+01  3e-14  3e-13\n",
      " 3: -2.9732e+02 -3.0737e+02  1e+01  6e-14  3e-13\n",
      " 4: -2.9972e+02 -3.0085e+02  1e+00  3e-14  3e-13\n",
      " 5: -2.9998e+02 -3.0011e+02  1e-01  1e-14  3e-13\n",
      " 6: -3.0000e+02 -3.0001e+02  5e-03  1e-14  4e-13\n",
      " 7: -3.0000e+02 -3.0000e+02  6e-05  2e-14  4e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0501e+02 -1.1180e+03  3e+03  8e-01  2e-13\n",
      " 1: -8.4019e+01 -4.3604e+02  6e+02  2e-01  1e-13\n",
      " 2: -5.5881e+01 -1.8133e+02  2e+02  6e-02  1e-13\n",
      " 3: -4.4592e+01 -9.2003e+01  8e+01  2e-02  8e-14\n",
      " 4: -4.1762e+01 -5.2657e+01  2e+01  3e-03  7e-14\n",
      " 5: -4.2641e+01 -4.7850e+01  7e+00  1e-03  7e-14\n",
      " 6: -4.3206e+01 -4.5509e+01  3e+00  4e-04  8e-14\n",
      " 7: -4.3784e+01 -4.4207e+01  4e-01  2e-06  8e-14\n",
      " 8: -4.3970e+01 -4.3984e+01  1e-02  6e-08  8e-14\n",
      " 9: -4.3976e+01 -4.3976e+01  2e-04  7e-10  8e-14\n",
      "10: -4.3976e+01 -4.3976e+01  2e-06  7e-12  1e-13\n",
      "Optimal solution found.\n",
      "Y :  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3]\n",
      "Y Prediciton:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "linear accuracy :  33.33333333333333\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8006e+01 -6.3575e+02  7e+02  9e-15  6e-16\n",
      " 1: -3.5892e+00 -6.2688e+01  6e+01  2e-15  1e-15\n",
      " 2: -8.3349e+00 -1.1570e+01  3e+00  8e-15  5e-16\n",
      " 3: -8.6789e+00 -9.4190e+00  7e-01  4e-15  2e-16\n",
      " 4: -8.7763e+00 -8.9175e+00  1e-01  4e-15  1e-16\n",
      " 5: -8.8000e+00 -8.8318e+00  3e-02  5e-16  2e-16\n",
      " 6: -8.8053e+00 -8.8139e+00  9e-03  7e-15  1e-16\n",
      " 7: -8.8073e+00 -8.8079e+00  6e-04  5e-15  2e-16\n",
      " 8: -8.8074e+00 -8.8074e+00  1e-05  4e-15  1e-16\n",
      " 9: -8.8074e+00 -8.8074e+00  2e-07  3e-15  2e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.7147e+00 -7.6214e+02  8e+02  9e-15  1e-15\n",
      " 1: -2.2623e+01 -9.3787e+01  7e+01  5e-15  1e-15\n",
      " 2: -3.1486e+01 -4.1127e+01  1e+01  2e-15  5e-16\n",
      " 3: -3.2605e+01 -3.4072e+01  1e+00  4e-15  3e-16\n",
      " 4: -3.2809e+01 -3.3139e+01  3e-01  4e-15  4e-16\n",
      " 5: -3.2867e+01 -3.2948e+01  8e-02  1e-14  4e-16\n",
      " 6: -3.2881e+01 -3.2899e+01  2e-02  2e-15  4e-16\n",
      " 7: -3.2885e+01 -3.2886e+01  8e-04  6e-15  4e-16\n",
      " 8: -3.2885e+01 -3.2885e+01  2e-05  1e-14  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.6488e+00 -7.9297e+02  8e+02  1e-14  1e-15\n",
      " 1: -2.4268e+01 -9.7436e+01  7e+01  2e-15  9e-16\n",
      " 2: -3.3406e+01 -4.5983e+01  1e+01  4e-16  6e-16\n",
      " 3: -3.4744e+01 -3.6821e+01  2e+00  2e-14  4e-16\n",
      " 4: -3.5010e+01 -3.5498e+01  5e-01  4e-16  3e-16\n",
      " 5: -3.5087e+01 -3.5184e+01  1e-01  7e-15  3e-16\n",
      " 6: -3.5108e+01 -3.5125e+01  2e-02  9e-15  4e-16\n",
      " 7: -3.5111e+01 -3.5112e+01  5e-04  2e-14  3e-16\n",
      " 8: -3.5112e+01 -3.5112e+01  1e-05  9e-16  3e-16\n",
      "Optimal solution found.\n",
      "Y :  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3]\n",
      "Y Prediciton:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 2 3 2 3 2 2 2 2 2 2 2 2 3 3 2 3 2 3 2 3 2\n",
      " 2 3 3 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3]\n",
      "rbf accuracy :  88.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = np.array(iris_data.species.astype('category').cat.codes.values) + 1 # Convert to integers because then changing the values will be easier (setosa - 1, versicolor = 2, and virginica = 3)\n",
    "x = iris_data.iloc[:, 0:4].to_numpy() # Get all of the columns except for the species(class) column\n",
    "\n",
    "def svm(x, y, ker):\n",
    "    # Create an array of [1, 2] values where the setosa class is 1 and all other classes have a classification of 2\n",
    "    y_1vsAll = np.ones(y.shape)\n",
    "    y_1vsAll[np.where(y != 1)] = 2\n",
    "    \n",
    "    # Create an array of [1, 2] values where the versicolor class is 1 and all other classes have a classification of 2\n",
    "    y_2vsAll = np.ones(y.shape)\n",
    "    y_2vsAll[np.where(y != 2)] = 2\n",
    "    \n",
    "    # Create an array of [1, 2] values where the virginica class is 1 and all other classes have a classification of 2\n",
    "    y_3vsAll = np.ones(y.shape)\n",
    "    y_3vsAll[np.where(y != 3)] = 2\n",
    "    \n",
    "    X = x[:, 2:4]\n",
    "\n",
    "    sv_indx = []  # support vectors index\n",
    "    \n",
    "    options.ker = ker  # use RBF kernel\n",
    "    options.arg = 0.15  # kernel argument\n",
    "    options.C = 3  # regularization constant\n",
    "    \n",
    "    model_1vsAll = train_svm(X.T,y_1vsAll.T,options)\n",
    "    yp_1vsAll,  prediction_1vsAll = classify_svm(X.T, model_1vsAll)\n",
    "    CA_1vsAll = len(np.where(y_1vsAll==yp_1vsAll))/len(y_1vsAll)\n",
    "    sv_indx.append(model_1vsAll.sv_indx)\n",
    "    \n",
    "    model_2vsAll = train_svm(X.T,y_2vsAll.T,options)\n",
    "    yp_2vsAll,  prediction_2vsAll = classify_svm(X.T, model_2vsAll)\n",
    "    CA_2vsAll = len(np.where(y_2vsAll==yp_2vsAll))/len(y_2vsAll)\n",
    "    sv_indx.append(model_2vsAll.sv_indx)\n",
    "    \n",
    "    model_3vsAll = train_svm(X.T,y_3vsAll.T,options)\n",
    "    yp_3vsAll,  prediction_3vsAll = classify_svm(X.T, model_3vsAll)\n",
    "    CA_3vsAll = len(np.where(y_3vsAll==yp_3vsAll))/len(y_3vsAll)\n",
    "    sv_indx.append(model_3vsAll.sv_indx)\n",
    "    \n",
    "    # Combine predictions and find the class with the maximum decision value\n",
    "    tmp = np.column_stack((prediction_1vsAll, prediction_2vsAll, prediction_3vsAll))\n",
    "    ypred = np.argmax(tmp, axis=1) + 1 \n",
    "    print(\"Y : \", y)\n",
    "    print(\"Y Prediciton: \", ypred)\n",
    "    accuracy = (sum(ypred == y) / 150) * 100\n",
    "    print(f'{ker} accuracy : ', accuracy)\n",
    "\n",
    "svm(x, y, 'linear')\n",
    "svm(x, y, 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response for part 3 \n",
    "The accuracy for the linear kernel svm is 33.33% and the accuracy for the rbf kernel svm is 88%. I believe that this is because the data for the iris dataset is not linearly separable. With that being said, the linear kernel attempts to find a linear separator between the different classes. Since this is difficult the accuracy is lower. The linear kernel has advantages because it is faster computationally and works well when there are a lot of dimensions. Since the irisi dataset tested only has 2 dimensions, it did not perform as well as the rbf kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<br><br>\n",
    "[1] Charu C. Aggarwal, Neural Networks and Deep Learning, Springer 2018<br><br>\n",
    "[2] Ahmad Abdolsaheb, How to make your Tic Tac Toe game unbeatable by using the minimax algorithm,\n",
    "2020, https://www.freecodecamp.org/news/how-to-make-your-tic-tac-toe-game-unbeatable-byusing-\n",
    "the-minimax-algorithm-9d690bad4b37/<br><br>\n",
    "[3] Francois Chollet, Deep Learning with Python, Manning, 2018<br><br>\n",
    "[4] Stephen Cook, The Complexity of Theorem Proving Procedures, Proceedings of the third annual ACM symposium<br><br>\n",
    "on Theory of computing, pp. 151-158, 1971\n",
    "[5] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016,\n",
    "https://www.deeplearningbook.org/<br><br>\n",
    "[6] Patric Honner (Contributing Columnist), Why Winning in Rock-Paper-Scissors (and in Life) Isn’t Everything,\n",
    "What does John Nash’s game theory equilibrium concept look like in Rock-Paper-Scissors?, an article in the\n",
    "online Quanta Magazine, April 2, 2018, https://www.quantamagazine.org/the-game-theory-math-behindrock-\n",
    "paper-scissors-20180402/<br><br>\n",
    "[7] Richard M. Karp, Reducibility Among Combinatorial Problems, In R. E. Miller and J. W. Thatcher (editors),\n",
    "Complexity of Computer Computations, New York: Plenum, pp. 85-103, 1972<br><br>\n",
    "[8] Stephen G. Nash and Ariela Sofer, Linear and Nonlinear Programming, McGraw-Hill, 1996<br><br>\n",
    "[9] Stuart Russell and Peter Norvig, Arti cial Intelligence a Modern Approach Fourth Edition, Pearson, 2020<br><br>\n",
    "[10] Sergios Theodoridis and Konstantinos Koutroumbas, Pattern Recognition Third Edition, San Diego, CA:\n",
    "Academic Press, 2006<br><br>\n",
    "[11] Thomas H. Cormen, Charles E. Leiserson, Ronal L. Rivest and Cli ord Stein, Introduction to Algorithms,\n",
    "3rd Edition, MIT Press, 2009<br><br>\n",
    "[12] David Zuckerman, NP-Complete Problems Have a Version That’s Hard to Approximate, IEEE, Proceedings\n",
    "of the Eighth Annual Structure in Complexity Theory Conference, pp. 305-312, 1993<br><br>\n",
    "[13] David Zuckerman, On Unapproximable Versions of NP-Complete Problems, SIAM Journal on Computing,\n",
    "Volume 25, Issue 6, pp. 1293-1304, 1996<br><br><br>\n",
    "[14] CVXOPT Documentation, Solving a Quadratic Program, https://cvxopt.org/examples/tutorial/qp.html<br><br>\n",
    "[15] StackOverflow, Python: porting to cvxopt quadratic programming from MATLAB's quadprog, https://stackoverflow.com/questions/20184207/python-porting-to-cvxopt-quadratic-programming-from-matlabs-quadprog<br><br>\n",
    "[16] Towards Data Science, Building a Convolutional Neural Network (CNN) in Keras, https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5<br><br>\n",
    "[17] Scikit-Learn Documentation, https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
